metric:
  behavior: toxicity
  metric: perspective
  lower_lim: 0.0
  upper_lim: 1.0
  dataset_name: allenai/real-toxicity-prompts


eval:
  epochs: 1
  num_samples: 20 #1e6
  num_bins: 10
  batch_size: 16

logging: 
  use_wandb: true
  api_key: # put your api key here
  entity: LLM_Accountability #richter-leo94 personal account

tau1: 
  model_id: gemma-1.1-7b-it #Mistral-7B-Instruct-v0.2
   # gemma-1.1-7b-it # Meta-Llama-3-8B-Instruct #LLMAccountability/Llama-3-8B-ckpt1 #Mistral-7B-Instruct-v0.2
    #model_id: LLMAccountability/Llama-3-8B-ckpt10
  gen_seed: seed1000
  model_kwargs:
    torch_dtype: torch.bfloat16
    load_in_4bit: true
    device_map: auto
  gen_kwargs:
    max_new_tokens: 100
    do_sample: true
    temperature: 0.7
    top_p: 0.9
  gen_batch_size: 8

tau2: 
  model_id: Mistral-7B-Instruct-v0.2
    #model_id: LLMAccountability/Llama-3-8B-ckpt10
  gen_seed: seed1000
  model_kwargs:
    torch_dtype: torch.bfloat16
    load_in_4bit: true
    device_map: auto
  gen_kwargs:
    max_new_tokens: 100
    do_sample: true
    temperature: 0.7
    top_p: 0.9
  gen_batch_size: 8


net:
  input_size: 1
  hidden_layer_size: [30, 30]
  layer_norm: true
  bias: true



train:
  seed: 0
  lr: 0.0005
  earlystopping:
    patience: 10
    delta: 0.0
  epochs: 100
  seqs: 60 # number of mini-batches
  T: 0 #Warm start number of mini-batches used for the training only
  alpha: 0.05 # significance level
  batch_size: 96
  save: false
  save_dir: ""
  l1_lambda: 0.0
  l2_lambda: 0.0
