metric:
  behavior: toxicity
  metric: perspective
  lower_lim: 0.0
  upper_lim: 1.0
  dataset_name: allenai/real-toxicity-prompts

epsilon: 0.01948 # neural net distance between tau1 and tau2 that we tolerate

eval:
  epochs: 1
  num_samples: 100000 #1e6
  num_bins: 10
  batch_size: 16
  use_vllm: false

logging: 
  use_wandb: true
  api_key: # put your api key here
  entity: LLM_Accountability #richter-leo94 personal account

analysis:
  calculate_distance: true
  unpaired: false
  num_runs: 20
  num_samples: 0
  multiples_of_epsilon: 3
  bias: 0
  use_full_ds_for_nn_distance: false


tau1: 
  model_id: program_execution-Meta-Llama-3-8B-Instruct
  #codealpaca-Meta-Llama-3-8B-Instruct
    #Llama-3-8B-ckpt1 #Mistral-7B-Instruct-v0.2
   # gemma-1.1-7b-it # Meta-Llama-3-8B-Instruct #LLMAccountability/Llama-3-8B-ckpt1 #Mistral-7B-Instruct-v0.2
    #model_id: LLMAccountability/Llama-3-8B-ckpt10
  gen_seed: seed1000
  model_kwargs:
    torch_dtype: torch.bfloat16
    load_in_4bit: true
    device_map: auto
    low_cpu_mem_usage: true
  gen_kwargs:
    max_new_tokens: 100
    do_sample: true
    temperature: 0.7
    top_p: 0.9
  gen_batch_size: 8
  hf_prefix: LLMAccountability #meta-llama

tau2: 
  model_id: Llama-3-8B-ckpt4
    #model_id: LLMAccountability/Llama-3-8B-ckpt10
  gen_seed: seed1000
  model_kwargs:
    torch_dtype: torch.bfloat16
    load_in_4bit: true
    device_map: auto
  gen_kwargs:
    max_new_tokens: 100
    do_sample: true
    temperature: 0.7
    top_p: 0.9
  gen_batch_size: 8
  hf_prefix: LLMAccountability


net:
  input_size: 1
  hidden_layer_size: [32, 32] #[30, 30]
  layer_norm: true
  bias: true
