# @package _global_

exp: test  # Specifies the type of experiment

# Override tau1 configurations
tau1:
  model_id: gemma-1.1-7b-it
  #Meta-Llama-3-8B-Instruct #Mistral-7B-Instruct-v0.2 #gemma-1.1-7b-it
  #Llama-3-8B-ckpt1 
  gen_seed: seed1000  # Specify the generation seed
  # All other tau1 parameters inherit from the base config.yaml

# Override tau2 configurations
tau2:
  model_id: gemma-1.1-7b-it
  #Meta-Llama-3-8B-Instruct #Mistral-7B-Instruct-v0.2 #gemma-1.1-7b-it
  #Llama-3-8B-ckpt1 #Meta-Llama-3-8B-Instruct_hightemp
  gen_seed: seed2000  # Specify the generation seed
  # All other tau2 parameters inherit from the base config.yaml

# Override metric configurations if needed
metric:
  behavior: toxicity  
  metric: perspective # toxicity for Roberta-based toxicity classifier, see https://huggingface.co/spaces/evaluate-measurement/toxicity
  lower_lim: 0.0                   
  upper_lim: 1.0                   
  dataset_name: allenai/real-toxicity-prompts  

test_params:
  only_continuations: true # whether to score only model generations or whole prompt + generation
  fold_size: 4000
  overwrite: false
  calibrate: false #true
  noise: 0 # noise for behavior scores


wandb_project_name: Test_Toxicity

calibration_params: 
  epsilon_strategy: default # std # interval # if calibrate==True, strategy to derive epsilon in test
  epsilon_ticks: 10 # if calibrate==True, number of epsilon values to try
  bias: 0 # if calibrate==True, bias for epsilon values
  lower_interval_end: 0 
  upper_interval_end: 0.2
  lower_model_name: Meta-Llama-3-8B-Instruct_hightemp
  lower_model_seed: seed1000
  upper_model_name: LLama-3-8b-Uncensored
  upper_model_seed: seed1000
  num_runs: 20

logging:
  use_wandb: false
